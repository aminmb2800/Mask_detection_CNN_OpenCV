{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face_mask_detection_CNN_OpenCV\n",
    "\n",
    "#### Author : Amin  4.03.2023\n",
    "\n",
    "#### Aim of the Project\n",
    "The dataset consists of 1376 images consisting of two classes– with_mask and without_mask. The objective to create a Face Mask Detector using CNN and OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we have the data set , we have to generate our pre-trained model: 'facemask_cnn_model.h5'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained models are pre-built neural network models that have already been trained on large datasets. These models have learned to recognize patterns and features in the data that they have been trained on. Pre-trained models can be used as a starting point for building custom models for specific tasks, which can save time and computational resources.\n",
    "In the case of the face mask detection model, the pre-trained model can recognize certain patterns and features in images that indicate the presence or absence of a face mask. Instead of starting from scratch, we can use a pre-trained model as a starting point and fine-tune it on our own dataset, which can lead to better performance and faster training times. Once the pre-trained model has been fine-tuned on our dataset, it can be saved as a .h5 file and used for face mask detection in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **haarcascade_frontalface_default.xml** file contains pre-trained models for detecting human faces in images and video streams. The file is part of the OpenCV library and is based on the Haar Cascade Classifier, which is a machine learning-based approach for object detection.\n",
    "In the context of the face mask detection model, we need to detect faces in real-time video streams so that we can determine whether a person is wearing a face mask or not. We use the haarcascade_frontalface_default.xml file along with the OpenCV library to detect faces in the video stream. Once we have detected a face, we can then pass the image of the face to the pre-trained model to determine whether the person is wearing a face mask or not.\n",
    "In summary, the haarcascade_frontalface_default.xml file is an important component of the face mask detection model as it allows us to detect human faces in real-time video streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• The first line imports the OpenCV library for image processing.\n",
    "\n",
    "• The second line imports the NumPy library for numerical operations.\n",
    "\n",
    "• The third line imports the Sequential class from the Keras library, which is used to create deep learning models.\n",
    "\n",
    "• The fourth line imports the Conv2D, MaxPooling2D, Flatten, and Dense classes from Keras, which are used to create layers in the model.\n",
    "\n",
    "• The fifth line imports ModelCheckpoint and LearningRateScheduler classes from Keras callbacks module, which are used to define custom behaviour during model training.\n",
    "\n",
    "• The sixth line imports train_test_split function from sklearn.model_selection module, which is used to split dataset into train and test set.\n",
    "\n",
    "• The seventh line imports math library for mathematical operations.\n",
    "\n",
    "• The eighth line imports os module which provides a way of using operating system dependent functionality.\n",
    "\n",
    "• The ninth line keras.models.load_model is a function that allows you to load a pre-trained Keras model from a file. It is used when you want to use a previously trained model for inference or further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ignore specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **These lines define constants to be used in the model. IMG_SIZE defines the size of the image, BATCH_SIZE defines the number of images to be processed in each batch, NUM_EPOCHS defines the number of epochs to train the model for, and MODEL_FILE is the filename to save the trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "MODEL_FILE = 'facemask_cnn_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This function defines a learning rate schedule that will be used during model training. The learning rate is set to 1e-3 initially, but after 5 epochs it is halved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 5:\n",
    "        lr *= 0.5\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **These lines load the image data from the directories 'with_mask' and 'without_mask' using OpenCV, resize them to the IMG_SIZE, and store them in NumPy arrays. Then the arrays are concatenated to create the full dataset, and split into training and testing sets using train_test_split function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "mask_images = np.array([cv2.resize(cv2.imread('with_mask/' + filename), (IMG_SIZE, IMG_SIZE)) for filename in os.listdir('with_mask')])\n",
    "nomask_images = np.array([cv2.resize(cv2.imread('without_mask/' + filename), (IMG_SIZE, IMG_SIZE)) for filename in os.listdir('without_mask')])\n",
    "X = np.concatenate((mask_images, nomask_images))\n",
    "y = np.concatenate((np.ones(len(mask_images)), np.zeros(len(nomask_images))))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **The Sequential class is a linear stack of layers that allows you to build a deep learning model. This line initializes an instance of the Sequential class and assigns it to the variable model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This adds a convolutional layer with 32 filters, each of size 3x3, and the ReLU activation function. The input_shape argument specifies the shape of the input to the layer, which is a 3D tensor of size (IMG_SIZE, IMG_SIZE, 3), where the last dimension represents the number of color channels (RGB).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This adds a max pooling layer with a pool size of 2x2, which reduces the spatial dimensions of the previous layer by a factor of 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **These lines add additional convolutional and max pooling layers, gradually increasing the number of filters and decreasing the spatial dimensions of the output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This flattens the output from the previous layer into a 1D array, which can be fed into a fully connected layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This adds a fully connected layer with 256 neurons and the ReLU activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This adds the output layer with a single neuron and the sigmoid activation function, which outputs a value between 0 and 1 representing the probability that the input belongs to the positive class (wearing a mask).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This compiles the model by specifying the optimizer to use (Adam), the loss function to optimize (binary cross-entropy), and the evaluation metrics to use (accuracy).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **These lines define two callback functions to be used during training. ModelCheckpoint saves the model with the highest validation accuracy during training, and LearningRateScheduler adjusts the learning rate according to a predefined schedule.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define callbacks\n",
    "checkpoint = ModelCheckpoint(MODEL_FILE, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This fits the model to the training data (X_train and y_train) for a specified number of epochs (NUM_EPOCHS) and batch size (BATCH_SIZE), using the validation data (X_test and y_test) to monitor the performance of the model during training. The callbacks argument specifies the callback functions to be used during training, including ModelCheckpoint and LearningRateScheduler.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 30.8564 - accuracy: 0.6673\n",
      "Epoch 1: val_accuracy improved from -inf to 0.82246, saving model to facemask_cnn_model.h5\n",
      "35/35 [==============================] - 102s 3s/step - loss: 30.8564 - accuracy: 0.6673 - val_loss: 0.3923 - val_accuracy: 0.8225 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3275 - accuracy: 0.8836\n",
      "Epoch 2: val_accuracy improved from 0.82246 to 0.84058, saving model to facemask_cnn_model.h5\n",
      "35/35 [==============================] - 101s 3s/step - loss: 0.3275 - accuracy: 0.8836 - val_loss: 0.3565 - val_accuracy: 0.8406 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1312 - accuracy: 0.9600\n",
      "Epoch 3: val_accuracy improved from 0.84058 to 0.93478, saving model to facemask_cnn_model.h5\n",
      "35/35 [==============================] - 99s 3s/step - loss: 0.1312 - accuracy: 0.9600 - val_loss: 0.1848 - val_accuracy: 0.9348 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5049 - accuracy: 0.8727\n",
      "Epoch 4: val_accuracy improved from 0.93478 to 0.95290, saving model to facemask_cnn_model.h5\n",
      "35/35 [==============================] - 119s 3s/step - loss: 0.5049 - accuracy: 0.8727 - val_loss: 0.1825 - val_accuracy: 0.9529 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9691\n",
      "Epoch 5: val_accuracy improved from 0.95290 to 0.96377, saving model to facemask_cnn_model.h5\n",
      "35/35 [==============================] - 121s 3s/step - loss: 0.0953 - accuracy: 0.9691 - val_loss: 0.0774 - val_accuracy: 0.9638 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9809\n",
      "Epoch 6: val_accuracy did not improve from 0.96377\n",
      "35/35 [==============================] - 118s 3s/step - loss: 0.0634 - accuracy: 0.9809 - val_loss: 0.1084 - val_accuracy: 0.9493 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9809\n",
      "Epoch 7: val_accuracy did not improve from 0.96377\n",
      "35/35 [==============================] - 108s 3s/step - loss: 0.0545 - accuracy: 0.9809 - val_loss: 0.0835 - val_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9936\n",
      "Epoch 8: val_accuracy did not improve from 0.96377\n",
      "35/35 [==============================] - 123s 4s/step - loss: 0.0249 - accuracy: 0.9936 - val_loss: 0.0720 - val_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9973\n",
      "Epoch 9: val_accuracy improved from 0.96377 to 0.96739, saving model to facemask_cnn_model.h5\n",
      "35/35 [==============================] - 114s 3s/step - loss: 0.0162 - accuracy: 0.9973 - val_loss: 0.0749 - val_accuracy: 0.9674 - lr: 5.0000e-04\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9982\n",
      "Epoch 10: val_accuracy improved from 0.96739 to 0.97826, saving model to facemask_cnn_model.h5\n",
      "35/35 [==============================] - 118s 3s/step - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.0653 - val_accuracy: 0.9783 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test), callbacks=[checkpoint, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Then we only need to save the pretained model in order to use it for the real time.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code loads a pre-trained convolutional neural network (CNN) model from a saved file and uses it to classify faces in the video frames. \n",
    "\n",
    "### The code then opens a video capture device and continuously loops through the frames, calling the detect_face_mask function to process each frame and displaying the result in a window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This line sets the IMG_SIZE constant to 224, which is the expected input size of the pre-trained CNN model.**\n",
    "* **The next line sets the MODEL_FILE constant to the name of the file containing the pre-trained CNN model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMG_SIZE = 224\n",
    "MODEL_FILE = 'facemask_cnn_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This line loads the pre-trained CNN model from the MODEL_FILE file using the load_model function from the Keras library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = load_model(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This code defines a function called detect_face_mask that takes a single argument frame, which is a frame from a video or webcam feed. The purpose of the function is to detect whether or not faces in the frame are wearing masks.**\n",
    "\n",
    "\n",
    "* **The first step of the function is to convert the input frame from a color image to a grayscale image using the OpenCV function cv2.cvtColor.**\n",
    "\n",
    "\n",
    "* **Next, the function uses a Haar cascade classifier to detect faces in the grayscale image. The classifier is initialized with the cv2.CascadeClassifier function, which loads a pre-trained XML file containing the classifier data. The detectMultiScale function is then called on the grayscale image to detect faces, with several parameters such as scaleFactor, minNeighbors, and minSize specified to control the sensitivity and accuracy of the detection.**\n",
    "\n",
    "\n",
    "* **For each face detected, the function extracts the region of interest (ROI) from the original color image, resizes it to a specified size using cv2.resize, and preprocesses the ROI for input to a machine learning model. The np.array function is used to convert the ROI to a NumPy array, which is then divided by 255.0 to normalize the pixel values to the range [0, 1]. The np.expand_dims function is used to add an extra dimension to the array to match the expected input shape of the model.**\n",
    "\n",
    "\n",
    "* **The machine learning model is then used to predict whether or not the face in the ROI is wearing a mask. The prediction is obtained by calling the model.predict function on the preprocessed ROI, and indexing the result to retrieve the actual prediction value.**\n",
    "\n",
    "\n",
    "* **Finally, the function draws a rectangle around the face in the original color image using cv2.rectangle, with the color of the rectangle determined by the prediction result. The label of the prediction is also written above the rectangle using cv2.putText, with the same color as the rectangle. The function returns the original color image with the rectangles and labels drawn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to detect face masks in a frame\n",
    "def detect_face_mask(frame):\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Use a Haar cascade classifier to detect faces in the grayscale image\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n",
    "\n",
    "    # For each face detected, classify whether or not it is wearing a mask\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region from the frame and resize it to the input size of the model\n",
    "        face_img = frame[y:y+h, x:x+w]\n",
    "        face_img = cv2.resize(face_img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        # Preprocess the face image for input to the model\n",
    "        face_array = np.array(face_img) / 255.0\n",
    "        face_array = np.expand_dims(face_array, axis=0)\n",
    "\n",
    "        # Use the model to predict whether or not the face is wearing a mask\n",
    "        prediction = model.predict(face_array)[0][0]\n",
    "\n",
    "        # Draw a rectangle around the face and label it with the prediction result\n",
    "        label = 'Mask' if prediction > 0.5 else 'No mask'\n",
    "        color = (0, 255, 0) if prediction > 0.5 else (0, 0, 255)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **This line initializes a video capture device with the index 0, which is usually the default camera on the computer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a video capture device\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **while True:: starts an infinite loop that will continuously process frames from the video capture device until the program is manually stopped or the device is disconnected.**\n",
    "\n",
    "\n",
    "* **ret, frame = cap.read(): reads a single frame from the video capture device cap and stores it in the variable frame. The return value ret is a Boolean indicating whether the frame was read successfully or not.**\n",
    "\n",
    "\n",
    "* **if not ret: break: checks whether the frame was read successfully. If it was not, the program exits the loop.**\n",
    "\n",
    "\n",
    "* **frame = detect_face_mask(frame): calls the detect_face_mask() function to detect face masks in the current frame.**\n",
    "\n",
    "\n",
    "* **cv2.imshow('Face Mask Detection', frame): displays the frame in a window named \"Face Mask Detection\".**\n",
    "\n",
    "\n",
    "* **if cv2.waitKey(1) & 0xFF == ord('q'): break: waits for a key press and checks if the pressed key is the 'q' key. If it is, the program exits the loop.**\n",
    "\n",
    "\n",
    "* **cap.release(): releases the video capture device, freeing up system resources.**\n",
    "\n",
    "\n",
    "* **cv2.destroyAllWindows(): closes all windows opened by OpenCV.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 246ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-95e75f12fa5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Detect face masks in the frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_face_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Display the frame in a window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-fd68be0f57ee>\u001b[0m in \u001b[0;36mdetect_face_mask\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Use a Haar cascade classifier to detect faces in the grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mface_cascade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'haarcascade_frontalface_default.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaleFactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminNeighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# For each face detected, classify whether or not it is wearing a mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through frames from the video capture device\n",
    "while True:\n",
    "    # Read a frame from the video capture device\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # If the frame was not read successfully, break out of the loop\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect face masks in the frame\n",
    "    frame = detect_face_mask(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Face Mask Detection', frame)\n",
    "\n",
    "    # Wait for a key press and check if the 'q' key was pressed to quit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture device and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
